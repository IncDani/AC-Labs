{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AC Labs 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cqauw_nL7wzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "np.set_printoptions(suppress=True)\n",
        "import math \n",
        "\n",
        "### Plotting parameters for the looks - Not neural network related ###\n",
        "plt.rcParams['figure.figsize'] = [18, 9]\n",
        "plt.rcParams['axes.labelsize'] = 18\n",
        "plt.rcParams['axes.labelcolor'] = 'w'\n",
        "plt.rcParams['axes.titlecolor'] = 'w'\n",
        "plt.rcParams['axes.titlesize'] = 20 \n",
        "plt.rcParams['legend.fontsize'] = 18"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDrwpamX72kB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Activation Function ###\n",
        "# Used to introduce nonlinearity in the outputs\n",
        "# Without this, the network won't be able to learn other functions than linear ones - linear regression that is\n",
        "# Used in each layer of the network, taking as input the weighted sum of the inputs computed in the layer\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCe8VQBO74MM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-6, 6, 100) # evenly spaced 100 numbers over a [-6, 6] interval\n",
        "sigmoid_plot_data = sigmoid(x)  # applies sigmoid function over x vector\n",
        "sigmoid_derivative_plot_data = sigmoid_derivative(x) # same but with the derivative of sigmoid\n",
        "\n",
        "### Plotting section ###\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "ax.plot(x, sigmoid_plot_data)\n",
        "ax.plot(x, sigmoid_derivative_plot_data)\n",
        "ax.grid()\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('f (x)')\n",
        "ax.legend(['Sigmoid', 'Sigmoid derivative'])\n",
        "ax.tick_params(axis='both', colors='w', labelsize='large')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7U-3bZl76M0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, x, y):\n",
        "\n",
        "        # Used to reproduce predictable results\n",
        "        np.random.seed(6)\n",
        "        \n",
        "        self.input      = x     # Input layer - Placeholder\n",
        "        self.y          = y     # Desired output - Placeholder\n",
        "        self.weights1   = np.random.rand(self.input.shape[1], self.input.shape[0])      # Weights for the hidden layer - Randomly initialized\n",
        "        self.weights2   = np.random.rand(self.y.shape[0], self.y.shape[1])              # Eights for the output layer - Randomly initialized\n",
        "                                                                                        # Tip: look at the shape of the weights - see how they match considering\n",
        "                                                                                        # matrix multiplication (done in feedforward)\n",
        "        self.output     = np.zeros(y.shape)     # Network predicted output - Initialized with 0\n",
        "\n",
        "        # List used to save loss values for plotting\n",
        "        self.history    = list()\n",
        "    \n",
        "    def feedforward(self):\n",
        "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))    # Computing the output of the hidden layer\n",
        "        self.output = sigmoid(np.dot(self.layer1, self.weights2))   # Computing the output of the output layer \n",
        "                                                                    # This is the network output, which should be as close to the real one (y) as possible\n",
        "\n",
        "        self.history.append(self.loss())    # Append loss value to the history list for plotting purposes\n",
        "\n",
        "    def backprop(self):\n",
        "\n",
        "        # we canâ€™t directly calculate the derivative of the loss function with respect to the weights and biases because the equation of the loss function\n",
        "        # does not contain the weights and biases. Therefore, we need the chain rule to help us calculate it.\n",
        "\n",
        "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
        "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
        "\n",
        "        # update the weights with the derivative (slope) of the loss function - Gradient Descent\n",
        "        # This is the step where the network actually learns\n",
        "        self.weights1 += d_weights1\n",
        "        self.weights2 += d_weights2\n",
        "\n",
        "    # Sum of squares error\n",
        "    # The function which shows us how well the network performs: the lower the loss score the better the network predicts\n",
        "    def loss(self):\n",
        "        sum = 0\n",
        "        for label, pred in zip(self.y, self.output):\n",
        "            sum += (label - pred) ** 2\n",
        "        return sum[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlG4WfNZ8i4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Dataset ###\n",
        "\n",
        "### Function to learn: XOR 3 inputs\n",
        "\n",
        "# Train Data - based on which the prediction is done\n",
        "input_array = np.asarray(\n",
        "    [[0, 0, 0],\n",
        "    [0, 0, 1],\n",
        "    [0, 1, 0],\n",
        "    [0, 1, 1],\n",
        "    [1, 0, 0],\n",
        "    [1, 0, 1],\n",
        "    [1, 1, 0],\n",
        "    [1, 1, 1]]\n",
        "    )\n",
        "\n",
        "# Train Labels - the desired outputs that our network has to learn\n",
        "output_array = np.asarray(\n",
        "    [[0],\n",
        "     [1],\n",
        "     [1],\n",
        "     [0],\n",
        "     [1],\n",
        "     [0],\n",
        "     [0],\n",
        "     [1]]\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3unShbE8o9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "network = NeuralNetwork(input_array, output_array)\n",
        "print(network.loss())\n",
        "\n",
        "### Training the network ###\n",
        "\n",
        "epochs = 10000  # Number of learning cycles -> 10000 * (feedforward + backpropagation)\n",
        "threshold_value = 0.001 # Stop the learning when the loss score is under this threshold\n",
        "                        # Not a necessary parameter to the training, just a feature\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    network.feedforward()\n",
        "    network.backprop()\n",
        "    if network.loss() <= threshold_value:\n",
        "        last_epoch = epoch\n",
        "        break\n",
        "\n",
        "print(network.output)\n",
        "print(network.loss())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}